<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[Grant's Garden]]></title>
        <description><![CDATA[Work and writing in progress]]></description>
        <link>https://garden.grantcuster.com</link>
        <image>
            <url>https://grant-uploader.s3.amazonaws.com/og-images/index.png</url>
            <title>Grant&apos;s Garden</title>
            <link>https://garden.grantcuster.com</link>
        </image>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 04 Sep 2024 14:50:20 GMT</lastBuildDate>
        <atom:link href="https://garden.grantcuster.com/rss.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Wed, 04 Sep 2024 14:50:20 GMT</pubDate>
        <managingEditor><![CDATA[Grant Custer]]></managingEditor>
        <item>
            <title><![CDATA[2024-09-03-09-37-47-Cyberdeck-glasses-edition]]></title>
            <description><![CDATA[# Cyberdeck: glasses edition

![The elements: mini PC, AR glasses, portable battery, bluetooth keyboard](https://grant-uploader.s3.amazonaws.com/2024-09-03-16-21-02-2000.jpg)

![Simulation of POV from the porch. A floating window over the world.](https://grant-uploader.s3.amazonaws.com/2024-09-03-16-21-46-2000.jpg)

After trying out a [DIY deck version](/2024-04-22-20-46-52/), I've redone my cyberdeck experiment using AR glasses. It's not perfect but I've been using it pretty consistently on subway commutes and porch nights.

## Parts list

The parts list - very much inspired by the [Cyberdeck subreddit](https://www.reddit.com/r/cyberDeck/)

- Melee mini PC, Intel N100, 8gb RAM - [amazon link](https://www.amazon.com/dp/B0CP3WV82R)
- Xreal air 2 pro AR glasses - [link](https://us.shop.xreal.com/products/xreal-air-2-pro)
- INIU 45w charger - [link](https://www.amazon.com/dp/B09GJQG5S3?ref_=pe_386300_442618370_TE_sc_as_ri_0)
- Technikable bluetooth keyboard - [link](https://www.boardsource.xyz/products/Technikable)

The big plus of the mini PC is it has a powered display port out that can run the glasses, and another usb-c in for the battery (or laptop charger) power.

Most of the time this all gets transported in Bellroy cross-body bag.

- Bellroy bag

![Contents in the bag](https://grant-uploader.s3.amazonaws.com/2024-09-03-16-22-25-2000.jpg)

This is an iteration of when I had a keyboard plugging into my iPhone. The [exposed keyboard had a look](https://garden.grantcuster.com/2024-03-11-00-36-36/), but is probably not good for long term durability.

## Software

For software it runs the same thing I've been running everywhere these days. Ubuntu server with most things managed using Nix home-manager. Sway for a window manager. It is great to have things in sync with my Linux laptop, and close to sync (through home-manager) with the Mac I use at work.

## Visibility

I run the glasses in monitor mode, so there's no eye-tracking or anything. The compatibility has been great (can plug and unplug just like any other monitor). Readability can be tough, especially when looking at corners. I have the display scaled up to 2x which basically solves readability at the cost of information density. Sway helps here, it feels great to have basically no system chrome in the way.

![Simulation screenshot, close to what it looks like in practice](https://grant-uploader.s3.amazonaws.com/2024-09-03-16-21-46-2000.jpg)

The glasses work well in shade and rooms. In bright sunlight one of the biggest challenges is actually reflection up from your chest/shirt. The glasses cover helps a lot with this, it'd be cool to have a version that didn't also block out all light. A good candidate for 3D printing maybe.

## The keyboard

The keyboard is kind of its own whole deal, if you're doing this set up and aren't already deep in ergo keyboard world (see [ErgoMech subreddit](https://www.reddit.com/r/ErgoMechKeyboards/)), just get a conventional bluetooth keyboard + trackpad combo. That said the keyboard (using the [miryoku layout](https://github.com/manna-harbour/miryoku)) is a lot of fun as a lap keyboard, and the mouse keys feature is good in a pinch. 

Bluetooth has been the biggest hiccup, though. Sometimes the connection is choppy, with lots of latency. I don't know enough to know if it's the keyboard broadcaster, the mini PC bluetooth receiver, the bluetooth software, or some combo. Will continue investigating that. If I'm having lots of trouble and going to be sat for a while I just plug it in and use as a USB keyboard since that works to.

## Uses

### Subway

So far I mostly use it on the subway to noodle on side projects or write things like this post. I have about 45 minutes on the train, so it's great to be able to use that time I'd otherwise probably dedicate to podcasts. I've sometimes pulled out a laptop on the subway, this is less obtrusive but probably looks weirder - still I hope it's mostly an enjoyable cyberpunk-ish curiosity for the other riders. No one has said anything to me about it yet.

### Porch

I've also been using it on the porch, especially when trying to do some writing after the baby's gone to bed. This works OK - it's pretty with the neighborhood visible in the background, but I also feel like I'm pushing myself to use it there. Part of using it is really appreciating the laptop form factor - the extra screen space (because not scaled), the ability to look at different parts of the screen without it following you around... I wouldn't be surprised if I shift back to that more. The glasses setup really shines for the portability. But it's also a matter of getting the kinks/friction out, it's possible if I get my software set up just the right way I'll reach for the glasses more.

### TV

![](https://grant-uploader.s3.amazonaws.com/2024-09-03-16-23-14-2000.jpg)

This doesn't involve the glasses but I realized I can plug the mini PC into the TV, it's kind of nice for an almost ambient coding experience, where I can drop in, change a few lines, then walk away and straighten something up. Maybe there's some fit with the [Ambient TV](https://garden.grantcuster.com/2024-08-19-18-17-26/) setup there.

### Monitor

It also works as a regular PC, I wasn't expecting to use it much this way, but it's nice if I'm in the process of working on something and need a change of scenery. The combined display port means it plugs right into a one-cord laptop dock.

The mini PC is so small it's almost like plugging in a cartridge. I wonder if there's some fun/interesting setup with multiple task-specific mini PCs you swap out.

### Entertainment

I haven't pursued it much but I also want to have things set up to watch videos, particularly on subway rides where I don't get a seat. MPV and the media controls on bluetooth headphones should get me most of the way I think.

## Do I recommend it 

The challenge with any of this homebrew deck stuff is it's going to have sharp edges. If you really want to do this it's doable, but it definitely has a time cost. I hope to continue to iterate and make things smoother - paving the way for more alternative setups.
]]></description>
            <link>https://garden.grantcuster.com/2024-09-03-09-37-47-Cyberdeck-glasses-edition</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-09-03-09-37-47-Cyberdeck-glasses-edition</guid>
            <pubDate>Tue, 03 Sep 2024 13:37:47 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-09-02-19-57-44-Artisty-and-AI-the-Model-Makers]]></title>
            <description><![CDATA[# Artistry and AI: The model makers

Some of the most interesting responses I saw to Ted Chiang's piece:

Mat Dryhurst [tweet](https://x.com/matdryhurst/status/1830204253102538920):

> The easy unlock re the cyclical AI art culture war that will seem obvious in retrospect is that the people creating models are artists
>
> The same rules apply as in Chiang's piece. Some intervene in a deliberate manner in every stage of the process, some make rote and inelegant choices 

and this [thread](https://x.com/timhwang/status/1830630445072961903) from Tim Hwang again emphasizing model making

> for chiang, good art is about *choices*, lots of them
>
> if choice is art, pretraining a model is absolutely an art, and fine-tuning nearly always is too
> 
> model architectures, dataset selection, evaluation structures are  all deeply matters of curation, taste, and aesthetics
>
> like many people, chiang gets lost in the “push button, get art” implementations we see in the market
> 
> he blunders here 
> 
> chiang mistakes these apps as reflecting something *inherent* in language models as a technology, rather than a design choice on the part of the designer
> 
>this is why the analogy connecting LLMs as an art form to gaming as an art form is so important
>
> game design is an artistic act, but whether or not the interactivity that game allows is artistically worthwhile is separate question, defined by choice on the part of the designer

These were _fun_, I like to imagine a world of weird models put forward as art objects to explore.

Does it seem like this is where things are headed though?

I haven't been thinking this way. I think because I've mainly been focused on using models in a programming context, where I'm looking for capabilities to fit into apps.

And the narrative so far has just been scaling things up further and further, so that all kinds of responses are available within a model, you just need to prompt yourself into it.

I'm trying to think of things where the model's 'personality' or at least 'angle' is interesting. I've seem people speak about Claude that way, but even if there are edges where you can see the personality there, it still seems to me it's in such a capability race that I can't see real aesthetic-based curation happening. Like I can't imagine them choosing to leave out certain data because it doesn't fit the vibe if the data would contribute to performance.

Or, like, Midjourney has/had an aesthetic, but it feels like their goal is/will be 'anything you want to create, in any style' that just seems to be the way these push.

Maybe the variety shakes out more after the big capability race settles a bit, maybe synthetic data gets all capabilities to a point where more curation is possible and wanted.

And the idea of curating a dataset to define a model, does have some nice analogies - it feels like breeding grape varieties for wine or something. Where your intelligence about adding just a hint of this edgy data to balance out a mostly sophisticated set - and then a tasting with notes like 'a hint of this subreddit'.

But in that example what is it you actually do with the model? Maaaybe it's a chatbot? No I think better if it's language then it's some sort of interactive storytelling - in which case the big thing it would need to be is distilled - so that you could see it's value immediately because the potential for slop is so high...

So it's fun to think about but gets very fuzzy.

To pin something for another day: part of the question is how to distinguish a subtle, artist-trained model/work versus just a huge model prompted into that space. Part of it maybe that you couldn't jailbreak out of the artist model? In that way maybe it is like a game design...







]]></description>
            <link>https://garden.grantcuster.com/2024-09-02-19-57-44-Artisty-and-AI-the-Model-Makers</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-09-02-19-57-44-Artisty-and-AI-the-Model-Makers</guid>
            <pubDate>Mon, 02 Sep 2024 23:57:44 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-09-01-21-16-36-Zoom]]></title>
            <description><![CDATA[# CSS Zoom

Redid CSS zoom method again. Hopefully a base to build on for future projects at https://github.com/GrantCuster/zoom

![Zooming in on the sloth](https://grant-uploader.s3.amazonaws.com/2024-09-01-21-15-17.gif)
]]></description>
            <link>https://garden.grantcuster.com/2024-09-01-21-16-36-Zoom</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-09-01-21-16-36-Zoom</guid>
            <pubDate>Mon, 02 Sep 2024 01:16:36 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-09-01-11-06-33-Ted-Chiang-Why-AI-isnt-going-to-make-art]]></title>
            <description><![CDATA[# Ted Chiang: Why AI isn't going to make art

More response writing! Here we go. Same rules, don't summarize, pick out what's interesting to me even if it leaves a bunch of other stuff out.

First his definition:

> art is something that results from making a lot of choices.

In the case of prompting to generate text or images, he argues, you are making very few choices.

> What I'm saying is that art requires making choices at every scale; the countless small-scale choices made during implementation are just as iportant to the final product as the few large-scale choices made during the conception. [...] the interrelationship between the large scale and the small scale is where the artistry lies.

This is part of why I like making prototypes, going through the process of implementation, lots of small choices show up to be made, some of them suggest rippling, larger choices.

> I doubt you could replace every sentence in a thriller with one that is semantically equivalent and have the resulting novel be as entertaining.

This actually seems like a doable generative experiment...

Another big distinction, AI is not a language user because:

> Language [...] requires an intention to communicate.

Made clear by the example of ChatGPT being unable to _mean_ it when it says "I'm happy to see you".

Ending is stirring but not completely rigorous.

I like it best as meta-question of why do we create art why do we use language? There he puts the focus on intention.

That seems right to me but also limited, if we were looking for sincere intention in art, art would quickly get repetitive. We are looking for novelty too, or a response to what's came before that appears as novelty, or originality. Truthfulness is part of it, I wonder what the rest is?

]]></description>
            <link>https://garden.grantcuster.com/2024-09-01-11-06-33-Ted-Chiang-Why-AI-isnt-going-to-make-art</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-09-01-11-06-33-Ted-Chiang-Why-AI-isnt-going-to-make-art</guid>
            <pubDate>Sun, 01 Sep 2024 15:06:33 GMT</pubDate>
        </item>
    </channel>
</rss>