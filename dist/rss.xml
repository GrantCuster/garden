<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[Grant's Garden]]></title>
        <description><![CDATA[Work and writing in progress]]></description>
        <link>https://garden.grantcuster.com</link>
        <image>
            <url>https://grant-uploader.s3.amazonaws.com/og-images/index.png</url>
            <title>Grant&apos;s Garden</title>
            <link>https://garden.grantcuster.com</link>
        </image>
        <generator>RSS for Node</generator>
        <lastBuildDate>Fri, 23 Aug 2024 01:45:07 GMT</lastBuildDate>
        <atom:link href="https://garden.grantcuster.com/rss.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Fri, 23 Aug 2024 01:45:07 GMT</pubDate>
        <managingEditor><![CDATA[Grant Custer]]></managingEditor>
        <item>
            <title><![CDATA[2024-08-22-13-34-23]]></title>
            <description><![CDATA[Testing incremental rebuild from the high line.
]]></description>
            <link>https://garden.grantcuster.com/2024-08-22-13-34-23</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-22-13-34-23</guid>
            <pubDate>Thu, 22 Aug 2024 13:34:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-21-20-44-19]]></title>
            <description><![CDATA[I think this is actually the 'right' progressive pixelation. Had to step away for a while and come back.

![Pixelated sloth redux](https://grant-uploader.s3.amazonaws.com/2024-08-21-20-44-01-2000.jpg)
]]></description>
            <link>https://garden.grantcuster.com/2024-08-21-20-44-19</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-21-20-44-19</guid>
            <pubDate>Wed, 21 Aug 2024 20:44:19 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-21-19-55-38]]></title>
            <description><![CDATA[Got month-based index pages working here. Along with infinite scroll loading. Still want to get incremental rebuild working. Then I'll feel better about doing lots of little posts here.
]]></description>
            <link>https://garden.grantcuster.com/2024-08-21-19-55-38</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-21-19-55-38</guid>
            <pubDate>Wed, 21 Aug 2024 19:55:38 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-20-19-33-27]]></title>
            <description><![CDATA[Finally wearing my AR glasses combo out on the commute. Made some decent progress trying to paginate this blog today. Pushed the scale up to 2x which is helping. Also think I've got a good base system set up (refined from reinstalling across a bunch of computers of all the time) and that's helping.
]]></description>
            <link>https://garden.grantcuster.com/2024-08-20-19-33-27</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-20-19-33-27</guid>
            <pubDate>Tue, 20 Aug 2024 19:33:27 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-19-18-17-26]]></title>
            <description><![CDATA[
Ambient TV is on the wall now.

![Flipping through channels on the ambient TV setup](https://grant-uploader.s3.amazonaws.com/2024-08-19-18-18-52.gif)
]]></description>
            <link>https://garden.grantcuster.com/2024-08-19-18-17-26</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-19-18-17-26</guid>
            <pubDate>Mon, 19 Aug 2024 18:17:26 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-19-09-55-41-Ambient-TV]]></title>
            <description><![CDATA[# Ambient TV

First ambient TV prototype is put together. Laptop strapped to a monitor with a macropad control. One button for random shuffle. Currently featuring Terry B bike riding, Primitive Technology and Rambalac. More iterations to come.

![Monitor showing ambient videos](https://grant-uploader.s3.amazonaws.com/2024-08-19-09-55-39.gif)
]]></description>
            <link>https://garden.grantcuster.com/2024-08-19-09-55-41-Ambient-TV</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-19-09-55-41-Ambient-TV</guid>
            <pubDate>Mon, 19 Aug 2024 09:55:41 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-18-08-04-04]]></title>
            <description><![CDATA[Pixelation layers

![Adjusting a pixelation layer on top of sloth image](https://grant-uploader.s3.amazonaws.com/2024-08-18-08-04-28.gif)
]]></description>
            <link>https://garden.grantcuster.com/2024-08-18-08-04-04</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-18-08-04-04</guid>
            <pubDate>Sun, 18 Aug 2024 08:04:04 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-13-21-32-02]]></title>
            <description><![CDATA[Doing some real system config work for the first time in a while. Trying to approach that 'sitting down to an empty page of paper' feeling again for sitting down to the computer. It is hard work to make things simple though.
]]></description>
            <link>https://garden.grantcuster.com/2024-08-13-21-32-02</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-13-21-32-02</guid>
            <pubDate>Tue, 13 Aug 2024 21:32:02 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-09-18-00-21]]></title>
            <description><![CDATA[Progressive pixelation.

![Sloth in layers of progressively smaller pixels](https://grant-uploader.s3.amazonaws.com/2024-08-09-17-58-45.gif)
]]></description>
            <link>https://garden.grantcuster.com/2024-08-09-18-00-21</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-09-18-00-21</guid>
            <pubDate>Fri, 09 Aug 2024 18:00:21 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-04-20-44-38]]></title>
            <description><![CDATA[OKLCH sloth variations

![Sloth hue winners in HSL and OKLCH](https://grant-uploader.s3.amazonaws.com/2024-08-04-20-47-10-800.jpg)
]]></description>
            <link>https://garden.grantcuster.com/2024-08-04-20-44-38</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-04-20-44-38</guid>
            <pubDate>Sun, 04 Aug 2024 20:44:38 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-02-12-42-40]]></title>
            <description><![CDATA[RGB winner versus RGB to HSL hue winner

![Sloth color variations](https://grant-uploader.s3.amazonaws.com/2024-08-02-12-42-26-800.jpg)

Hue winner kinda looks like thermal camera.
]]></description>
            <link>https://garden.grantcuster.com/2024-08-02-12-42-40</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-02-12-42-40</guid>
            <pubDate>Fri, 02 Aug 2024 12:42:40 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-01-22-32-05]]></title>
            <description><![CDATA[Better pixel glooper. One effect I like here and in palette reduction generally is when the background and foreground start to infect eachother.

![Forest enveloping a sloth](https://grant-uploader.s3.amazonaws.com/2024-08-01-22-32-31-800.jpg)
]]></description>
            <link>https://garden.grantcuster.com/2024-08-01-22-32-05</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-01-22-32-05</guid>
            <pubDate>Thu, 01 Aug 2024 22:32:05 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-08-01-18-18-51]]></title>
            <description><![CDATA[Trying out flood fill averaging agents. Start in a random spot and eat nearby color values, digest as average.

![Sloth rendered in patches](https://grant-uploader.s3.amazonaws.com/2024-08-01-18-19-55-800.jpg)
]]></description>
            <link>https://garden.grantcuster.com/2024-08-01-18-18-51</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-08-01-18-18-51</guid>
            <pubDate>Thu, 01 Aug 2024 18:18:51 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-07-29-19-41-42]]></title>
            <description><![CDATA[
Some debug images:

![Sloth to outlines](https://grant-uploader.s3.amazonaws.com/2024-07-29-19-42-55-2000.jpg)

![Sloth to blocks](https://grant-uploader.s3.amazonaws.com/2024-07-29-19-43-21-2000.jpg)

![Me to outlines](https://grant-uploader.s3.amazonaws.com/2024-07-29-19-42-21-2000.jpg)
]]></description>
            <link>https://garden.grantcuster.com/2024-07-29-19-41-42</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-07-29-19-41-42</guid>
            <pubDate>Mon, 29 Jul 2024 19:41:42 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-07-28-22-29-19]]></title>
            <description><![CDATA[
Generally working. Need to think more on where to take it.

![Another sloth shot](https://grant-uploader.s3.amazonaws.com/2024-07-28-22-30-15-800.jpg)

![Progressively pixelated Olympic shot](https://grant-uploader.s3.amazonaws.com/2024-07-28-22-30-34-2000.jpg)

Part of the question will be how to get people to look closer and realize the artifacts are deliberate.
]]></description>
            <link>https://garden.grantcuster.com/2024-07-28-22-29-19</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-07-28-22-29-19</guid>
            <pubDate>Sun, 28 Jul 2024 22:29:19 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-07-28-20-16-02]]></title>
            <description><![CDATA[Debug outtake.

![Debugging the sloth with accidental triangles](https://grant-uploader.s3.amazonaws.com/2024-07-28-20-16-30-2000.jpg)
]]></description>
            <link>https://garden.grantcuster.com/2024-07-28-20-16-02</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-07-28-20-16-02</guid>
            <pubDate>Sun, 28 Jul 2024 20:16:02 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-07-28-18-32-48]]></title>
            <description><![CDATA[Progressive resolution based on difference from average. It's not wrong but it's not quite right.

![Sloth compression attempt](https://grant-uploader.s3.amazonaws.com/2024-07-28-18-33-49-800.jpg)
]]></description>
            <link>https://garden.grantcuster.com/2024-07-28-18-32-48</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-07-28-18-32-48</guid>
            <pubDate>Sun, 28 Jul 2024 18:32:48 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-07-28-11-47-10]]></title>
            <description><![CDATA[256 colors seems like it should be enough
]]></description>
            <link>https://garden.grantcuster.com/2024-07-28-11-47-10</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-07-28-11-47-10</guid>
            <pubDate>Sun, 28 Jul 2024 11:47:10 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-07-26-17-50-11]]></title>
            <description><![CDATA[Thumbs up / thumbs down

![Screen recording of demo app where it records if I gave a thumbs up or thumbs down](https://grant-uploader.s3.amazonaws.com/2024-07-26-17-49-51.gif)
]]></description>
            <link>https://garden.grantcuster.com/2024-07-26-17-50-11</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-07-26-17-50-11</guid>
            <pubDate>Fri, 26 Jul 2024 17:50:11 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-07-25-19-44-00]]></title>
            <description><![CDATA[Finished listening to the Diamond Age. Pretty fun! The primer was maybe a little different than what I expected from hearing it referenced. But man I will always love an experience that secretly teaches you how to run it yourself.  Need to read the Andy Matuschak essay now.
]]></description>
            <link>https://garden.grantcuster.com/2024-07-25-19-44-00</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-07-25-19-44-00</guid>
            <pubDate>Thu, 25 Jul 2024 19:44:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-07-24-17-07-32]]></title>
            <description><![CDATA[Erosion becomes collapse

![GIF of collapsing image pixeles](https://grant-uploader.s3.amazonaws.com/2024-07-24-17-07-08.gif)

]]></description>
            <link>https://garden.grantcuster.com/2024-07-24-17-07-32</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-07-24-17-07-32</guid>
            <pubDate>Wed, 24 Jul 2024 17:07:32 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-07-24-16-56-56]]></title>
            <description><![CDATA[
Redid the way I do threads on this garden. Now a separate file with newline separted posts. Cautiously optimistic. Still more work to be done to make posting here feel easy.
]]></description>
            <link>https://garden.grantcuster.com/2024-07-24-16-56-56</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-07-24-16-56-56</guid>
            <pubDate>Wed, 24 Jul 2024 16:56:56 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-07-08-10-19-55-About-Constraint-Systems]]></title>
            <description><![CDATA[# Constraint Systems draft

I'm going to give a work presentation on Constraint Systems, going to start drafting out the pieces here:

Constraint Systems is a series of side projects I've been working on for about five years now.

It's a collection of 30 alternative interfaces for creating and editing images and text.

The tools are focused on engaging with digital as a material. Whether it is pixels, text, or CSS.

I started working on these tools when I was mainly doing data visualization for a day job. Data visualization shares a lot of techniques with creative web tools, in terms of how you draw and update objects, but it was still a big jump for me to move to something where the user could draw on the screeen. It's a jump I'd been wanting to make for a long time though.tools,

I also had a lot of experience and ideas about how to make and present prototyhpes. Specifically my day job involved making a lot of prototypes, but had a lot of barriers to releasing them. I had some ideas about how to build practice around regularly releasing prototypes.

The first prototype, a simple canvas drawing app, was directly inspired by Hundred Rabbits work. They're a couple who live on a sailboat and make experimental code and art. They had done this drawing tool noodle, which was hyper-minimalist one-bit art drawing tool. And then they used it to make art everyday.

I'm still trying to pinpoint why I'm so interested in minimalist stuff, partly it is about aesthetics. I think a bigger point is that I like to see stuff that exposes how it works. That gives you a mental model of how it works as you're doing.

Something where you can say "I bet this is how this works" and you often turn out to be right.

Treating digital as a material is tricky, since computers are abstracting, "anything" machines. But they have a grain to them. If you try and trace them back to their primitives you get interestingb results. Not necessarily the 'true' primitives, but interesting ones.

Primitives I return to a lot:

- Canvas as a collection of pixels. Screen as a collection of pixsels. What you see on a computer screen is a grid of pixels. There's no denying that and I can't imagine that goes away anytime soon.
- Writing as a set of words, letters, sentences, and paragraphs. Ultimately that is all down to characters, and how those characters are laid out in sequence.

Flipping back and forth between those two concepts drives a lot of the exsperiments.

A screen is a collection of pixels: specificallyh a grid of RGBA values, ranging from 0-255.

I usually work in canvas, which feels natural to me. I don't get too much into shaders, which are arguably more natural at this pont? This is where digital as material is tricky/, I'm owrking several abstractions up.

Many of the experiences grew out of wanting to try out a specific technology more. Flow is probably the most popular, it mainly came out of realizing that the canvas drawImage function is very fast. It layers a bunch on top of each other. It's also an exercise in not overpreparing. You can bog it down by creating too many flows, I just, don't worry about that. Which is hopefully a signal of trust in some ways.

I also really want people to have control of their environments. I'm fascinated by times where you were allowed to really skin your OS. Leading to things like Hot Dog stand. Limiting people to a pretty palette can be limiting of self expression. There are lots of good reasons to have limits, accessibility and safety being main ones. But need to be balanced at times with giving user agency. Face is an exaqmple, hwere you can edit both the font and the text.

Something about this reminds me of play where you can see all of the elements you have to play with. You can think about how to combine them. The computer can hide elements for you, to save space and make things simpler. But it makes things feel brittle somehow. Like you're not on solid ground. I want to be on solid ground.

Especially in the early days, I thought about the text editor Vim inrelation to this. Vim is a text editor where there are distinct modes. By default you're in 'normal' mode where every key is a kind of shortcut for navigating or editing texxt. This system is now over 40 years old. It is interesting to me because it gives a sense of stability. It also is one of the most successful alternative methods for editing text. It has a big learning curve -- the cost of any alternative system, but people have found it valuable and/or interesting enough to keep at.
]]></description>
            <link>https://garden.grantcuster.com/2024-07-08-10-19-55-About-Constraint-Systems</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-07-08-10-19-55-About-Constraint-Systems</guid>
            <pubDate>Mon, 08 Jul 2024 10:19:55 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-30-19-58-04-Evolution-generation]]></title>
            <description><![CDATA[# Evolution generation

A lot of us are trying to figure out how to use LLMs as a creative tool. There's a set of projects related to creative coding that I've been thinking about lately and want to round up.

## SerenedipityLM

[Samin](https://x.com/samim) recenterly detailed their project [SerendipityLM](https://samim.io/studio/work/serendipityLM/) that focuses on "interactive evolutionary exploration of generative design models". It features a selection of generative art, mainly shader examples, generated through their process.

It highlights picbreeder. A non-AI image generation app, where users effectively went on a branching scavenger hunt through the possibility space. The [linked slides write-up](https://wiki.santafe.edu/images/3/34/Stanley_innovation_workshop14.pdf) is interesting.

The process is admirably clear. It starts with a prompt. Experiments are generated and then ranked and critiqued by the user. Those ranking are used to generate new exxamples in an interative process.

## Spellburst

[Spellburst](https://spellburstllm.github.io/) is a paper and video demo made by [Tyler](https://x.com/tylerangert) and collaborators. It'd been a while since I'd looked at Spellburst and I think it really gets a lot of things right in terms of accomodating different levels of control for different parts of the process.

There's:

- prompts: for starting out and trying out conceptual changes in the creative code sketches. This is nice because these are often the changes that are tedious to try out quickly in code

- parameter tweaking: a dat.gui-like interface for tweaking experiment parameters. Sometimes you do want to tweak values specifically and interactively, and input elements like sliders are definitely a better interface for this than a prompt

- editing code: for real fine-tuned changes there's no substitute for direct code editing.

All of this is laid out in a node and wire interface that easily allows multiple versions and branches.

## Clicksynth and River

[Clicksynth](https://clicksynth.com) by Max Bittker lets you click through generated shaders to explore themes. Clicking into one gives you additional themes to steer it towards.

[River](https://maxbittker.com/river-notes) is similar, but focused on exploring a dataset. Clicking an image sends you into a space of similar images. The sound effects are very satisfying.

## Exploring a space

Looking over these, I think a lot about how AI generation is about exploring a space. Even a chat transcript is about steering the model into a space.

For my own prototypes, it makes me think about experimenting more with the kind of transcripts I send to the model. I'm starting to see transcripts (which can be procedurally edited or generated) more as a tool, rather than an actual conversation record.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-30-19-58-04-Evolution-generation</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-30-19-58-04-Evolution-generation</guid>
            <pubDate>Sun, 30 Jun 2024 19:58:04 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-30-16-34-30-Erosion]]></title>
            <description><![CDATA[# Erosion

Turns out a self-healing image looks a lot like a fading paint brush. More to try.

![GIF of an erosion experiment](https://grant-uploader.s3.amazonaws.com/2024-06-30-16-33-55.gif)

GIFs are probably not going to be kind to this series.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-30-16-34-30-Erosion</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-30-16-34-30-Erosion</guid>
            <pubDate>Sun, 30 Jun 2024 16:34:30 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-28-11-37-36]]></title>
            <description><![CDATA[Notebooks (Jupyter, Observable) are another related pattern here. Maybe a noetbook like linear flow but with horizontally scrolling generated functions...

]]></description>
            <link>https://garden.grantcuster.com/2024-06-28-11-37-36</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-28-11-37-36</guid>
            <pubDate>Fri, 28 Jun 2024 11:37:36 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-28-10-51-26]]></title>
            <description><![CDATA[This pattern actually is pretty similar to the flow for Copilot-like autocomplete. But I'd be interested in taking it out of the editor and also making the encapsulation clearer.

I guess the risk is the usual spaghetti-hairball nodes-and-wires issues. I should develop some sort of plan for avoiding that.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-28-10-51-26</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-28-10-51-26</guid>
            <pubDate>Fri, 28 Jun 2024 10:51:26 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-28-09-52-07-Thinking-about-codegen]]></title>
            <description><![CDATA[# Thinking about codegen

In recent LLM model experiments I've been thinking a lot about how to get codegen into the workflow. For a lot of tasks, conventional (javascript) code would be faster, cheaper and less prone to error than an LLM call. I'd like to have the LLM generate that code once -- then the game becomes how to encapsulate, verify and sometimes modify the code it generates.

## Some inspiration:

### Future of Coding: Pygmalion

[The Pygmalion podcast episode of Future of Coding](https://futureofcoding.org/episodes/072). This keeps turning over in my head. Particularly the 'code by example' bits, and the partial execution. Feels to me like there's some loop for an LLM to try generating a call, and then if the result doesn't parse or pass some verification process, it opens up a teach-by-example workflow.

### Parameter tweaking

![](https://grant-uploader.s3.amazonaws.com/2024-06-28-10-14-02-800.jpg)

[Silicon Jungle exploring parsing generated code for parameters](https://x.com/JungleSilicon/status/1806415376512893255). This has ties to parametric UI like dat.gui. I also see people generating things like this with Claude artifacts. Maybe parametric tweakability is another piece of the puzzle.

### Reactive flows

![](https://grant-uploader.s3.amazonaws.com/2024-06-28-10-15-21-800.jpg)

[Orion Reed on stitching elements reactively together](https://x.com/OrionReedOne/status/1805602416659673116). This is a slightly different path, I think, but lots of great patterns.

## Starting point

I need to pick some simple-but-not-too-simple task, and start working through the workflow.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-28-09-52-07-Thinking-about-codegen</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-28-09-52-07-Thinking-about-codegen</guid>
            <pubDate>Fri, 28 Jun 2024 09:52:07 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-25-20-53-23]]></title>
            <description><![CDATA[
It varies a lot day-to-day but I think it's safe to say I'm mildly stressed about the overall impact of AI. I've worked with machine learning for a long time now -- falling into with Fast Forward Labs ten years ago.

At their best, I think current developments encourage me to think about what I want -- out of work, out of computers. There's parts of some of my past work where I think the idea was part of what made it interesting to others, but also a big part was that I put in more work (as the Penn and Teller quote goes) than a reasonable person was willing too. The need to put in work in that specific way seems to be going away (the code could be generated instead). 

There's still places to put in the work: [carving the joints](https://garden.grantcuster.com/2024-05-31-07-16-31-Carving-at-the-joints/) is partly me trying to work that out. In figuring out how to put systems together, and the experience of that past work will aid in that... but it does feel like it will be different.

Maybe part of it is just more career-based. Somewhere in my head I think I had software developer as relatively interesting work I could always fall back on. I don't think it's going away, but I don't think it will continue as it has been either.

Stopping for tonight but I want to keep exploring and drilling into this.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-25-20-53-23</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-25-20-53-23</guid>
            <pubDate>Tue, 25 Jun 2024 20:53:23 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-25-16-42-08]]></title>
            <description><![CDATA[Riding home on the train. The repo on my work computer isn't up-to-date, but since these are all markdown files I can write and trust it will work when I upload later.

A bit of a dip in my new writing habit, mostly because my parents were in town for the weekend. Also maybe have some sort of lingering illness. Hoping it's just a temporary blip. I'm still enjoying the idea of this blog.

Been trying to take some stock of where my work and projects are at. The pace of AI experimentation feels a bit exhausting, and a lot of it feels like hype. At the same time there do appear to be some movements towards steerability, to interaction at a more feature-based and interpretable level that are interesting to me. I'm still looking to carve at the joints and put the right pieces together in the right order to make something that feels empowering and extended, rather than a replacement. But it varies day-to-day how achievable and likely I think that is.

One thing I'd like to explore more is how I should treat AI advancements in relation to kind of daily programming. I think Maggie Appleton mentioned feeling like focusing on craft in web engineering now feels a bit like you're working in the twilight of an era.

That fits with something I heard at the _Infinite Wonderland_ event with Google. One of the artists, I think it was Shawna X, talked about an issue with AI speeding up the creation process too much. Where you didn't have this kind of ramp up time with an idea, where you're doing something like sketching or picking colors (or in my case I feel like the equivalent is kind of getting a project repo ready, getting everything structured, etc.). Your brain is often processing the larger project in the background during this period. There's this nice feeling of kinding of sharpening your tools as the shape of the thing hopefully starts to untangle a bit in your head.

If you can go from idea to prototype (or even artifact) too fast do you lose that valuable processing time?

Of course, as was brought up at the event, there's nothing stopping you from doing things the old way, from spinning up just how you want to. And the perspective that emerges from that may be even more valuable if others are skipping it. Everything should shake out. But it's hard to trust in the sort of deluge of things created (but that could make it all the more valuable to do).
]]></description>
            <link>https://garden.grantcuster.com/2024-06-25-16-42-08</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-25-16-42-08</guid>
            <pubDate>Tue, 25 Jun 2024 16:42:08 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-19-10-41-31-Knowing-how-to-do-things]]></title>
            <description><![CDATA[# Knowing how to do things

Two comedy clips I think about surprising often. Tim Robinson on how not everyone knows everything:

<figure><video width="640" height="480" controls src="https://grant-uploader.s3.amazonaws.com/2024-06-19-10-35-26.mp4" type="video/mp4" poster="https://grant-uploader.s3.amazonaws.com/2024-06-19-10-49-31-800.jpg"></video></figure>

Harris Wittels on not being impressed by juggling:

<figure><audio controls src="https://grant-uploader.s3.amazonaws.com/2024-06-19-10-40-47.mp3" type="audio/mp3"></audio></figure>

If you know how to do something it is because you learned how to do it. If you don't know how to do something it's because you haven't learned how to do it.

Kind of comforting.

(Clips from _I Think You Should Leave_ driving sketch and _Comedy Bang Bang_ Farts and Procreation 4.)
]]></description>
            <link>https://garden.grantcuster.com/2024-06-19-10-41-31-Knowing-how-to-do-things</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-19-10-41-31-Knowing-how-to-do-things</guid>
            <pubDate>Wed, 19 Jun 2024 10:41:31 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-17-20-21-46]]></title>
            <description><![CDATA[Now writing this on the porch with the AR glasses. I think I finally got Network Manager set up so I feel more confident I can take these and connect to wifi outside of my own. Maybe I'll take them on their first train ride tomorrow.

I'm happy I've gotten as far as I did on this blog but I'm not quite sure how to use threads and untitled posts yet. Untitled posts feel weird even though part of the goal was to normalize that. Threads feel weird to, like it feels like I'm really declaring something important for it to be a thread. I want it to feel lightweight.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-17-20-21-46</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-17-20-21-46</guid>
            <pubDate>Mon, 17 Jun 2024 20:21:46 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-17-08-55-11]]></title>
            <description><![CDATA[
Writing this from the backyard using the AR glasses plus the mini PC in the cross-body bag. Trying to get more of a feel for it. Definitely still a lot of wires to track, and I don't trust things like the bluetooth keyboard to work all the time. The transparent mode is pretty, though.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-17-08-55-11</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-17-08-55-11</guid>
            <pubDate>Mon, 17 Jun 2024 08:55:11 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-16-19-33-19-Inspiration-Meeting-Mr-Kid-Pix]]></title>
            <description><![CDATA[# Inspiration: Meeting Mr. Kid Pix

<figure><video width="640" height="480" controls src="https://grant-uploader.s3.amazonaws.com/2024-06-16-19-32-10.mp4" type="video/mp4" poster="https://grant-uploader.s3.amazonaws.com/2024-06-17-23-32-17-800.jpg"></video></figure>

I really enjoyed [Meeting Mr. Kid Pix](https://www.youtube.com/watch?v=csalhuSixQU) by jeffrey aka [Whistlegraph on Twitter](https://x.com/whistlegraph). I appreciated the sincerity of both him and Craig Hickman. So nice to see people putting effort to understand + be understood.

This does touch on something I've tried to nail down before in regard to creative tools and video games.

If Kid Pix is so delightful (it is) what does it mean that it is a delightful paint program? Rather than a delightful video game?

Even if the produced image isn't the point, that you're manipulating an image is some part of it. That you see images all around you and now you're enjoying making them. It's got to be (I think) something to do with feeling agency. Video games give you agency too, but with a closed world (that's oversimplifying).

I can't fully articulate it! But it seems useful to keep returning to.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-16-19-33-19-Inspiration-Meeting-Mr-Kid-Pix</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-16-19-33-19-Inspiration-Meeting-Mr-Kid-Pix</guid>
            <pubDate>Sun, 16 Jun 2024 19:33:19 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-16-08-43-38]]></title>
            <description><![CDATA[It would be cool to make using the garden a fullscreen terminal experience. Fzf menu is already pretty close, maybe I just need like an ASCII art welcome message, and I should clear terminal history on menu open.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-16-08-43-38</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-16-08-43-38</guid>
            <pubDate>Sun, 16 Jun 2024 08:43:38 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-16-07-42-21]]></title>
            <description><![CDATA[
I have the fzf menu working now. It looks like this:

![Screenshot of my fzf garden menu](https://grant-uploader.s3.amazonaws.com/2024-06-16-12-35-57-2000.jpg)
]]></description>
            <link>https://garden.grantcuster.com/2024-06-16-07-42-21</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-16-07-42-21</guid>
            <pubDate>Sun, 16 Jun 2024 07:42:21 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-15-21-10-19]]></title>
            <description><![CDATA[I added month section subheaders to the main page and changed the timestamp display. I think I am mostly happy with it though I'm a bit concerned I'm getting too fiddly.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-15-21-10-19</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-15-21-10-19</guid>
            <pubDate>Sat, 15 Jun 2024 21:10:19 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-15-08-16-42-Garden-TODO]]></title>
            <description><![CDATA[# Garden TODO

- Sloth favicon (in green?)
- Bash + fzf admin script
    - build
    - build with images
    - commit and push
    - deploy (really just subtree push)
    - pull rebase
    - new_post
    - dev (run local server)
- Figure out partial/diffed rebuild
- Figure out pagination (big topic)
- Multiple image post styling (Twitter style)

]]></description>
            <link>https://garden.grantcuster.com/2024-06-15-08-16-42-Garden-TODO</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-15-08-16-42-Garden-TODO</guid>
            <pubDate>Sat, 15 Jun 2024 08:16:42 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-15-07-31-52-It-happened-to-me]]></title>
            <description><![CDATA[# It happened to me

I was writing a lot and then I decided I needed to redo the blog and then I thought I can't write more until I finish redoing the blog and so I wasn't writing.

So I pushed this version. It's more like Twitter, with individual entries and also the ability to thread. There are some unaswered questions (like pagination). I also can't decide if I should move all my feed posts in here. They might overwhelm the writing posts, but I think I'd like to figure out a way everything can live together.

I changed from bash scripts to typescript scripts, which has been helpful for organization. It's going well but I can also feel it in danger of sprawling out of what I can hold in my head -- and I feel like that is dangerous for solo projects, it's where it starts getting harder to jump back in and tinker. I should think about consolidating and carving up the pieces to make it easier to work on pieces at a time without feeling the stress that the whole thing might break.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-15-07-31-52-It-happened-to-me</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-15-07-31-52-It-happened-to-me</guid>
            <pubDate>Sat, 15 Jun 2024 07:31:52 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-11-18-02-59]]></title>
            <description><![CDATA[Thinking more about an RGB physical slider
]]></description>
            <link>https://garden.grantcuster.com/2024-06-11-18-02-59</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-11-18-02-59</guid>
            <pubDate>Tue, 11 Jun 2024 18:02:59 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-11-16-27-39]]></title>
            <description><![CDATA[
Most of the point of HTML is hyperlinks.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-11-16-27-39</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-11-16-27-39</guid>
            <pubDate>Tue, 11 Jun 2024 16:27:39 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-11-16-24-47]]></title>
            <description><![CDATA[
Editing this on the train. It would be good to have more posts to test with.

I should probably move in the garden posts.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-11-16-24-47</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-11-16-24-47</guid>
            <pubDate>Tue, 11 Jun 2024 16:24:47 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-09-21-58-15]]></title>
            <description><![CDATA[@reply 2024-06-09-20-57-07

Is this good syntax for a thread?
]]></description>
            <link>https://garden.grantcuster.com/2024-06-09-21-58-15</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-09-21-58-15</guid>
            <pubDate>Sun, 09 Jun 2024 21:58:15 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-09-20-57-07]]></title>
            <description><![CDATA[
Setting up my stream.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-09-20-57-07</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-09-20-57-07</guid>
            <pubDate>Sun, 09 Jun 2024 20:57:07 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-08-14-44-53-Blog-post-gif-slideshow]]></title>
            <description><![CDATA[# Blog post gif slideshow

Seeing the OG images I generate for this blog has me thinking. If you split a blog post by paragraphs and generated PNGs, and then you stitched those into a GIF with the frame time based on text length... that would maybe be a portable / embeddable blog post? Not good for accessibility and not to replace the post, but maybe an interesting derivative.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-08-14-44-53-Blog-post-gif-slideshow</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-08-14-44-53-Blog-post-gif-slideshow</guid>
            <pubDate>Sat, 08 Jun 2024 14:44:53 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-08-14-22-12-How-this-works]]></title>
            <description><![CDATA[# How this works

This site is a bash build script that concats HTML together. I also put together a little start script using fzf that has some common task options new_post / build / commit. I really like the feel of the starting bash script - it makes it feel like writing in this blog is its own little dedicated computer program.

The source for this one is viewable [on github](https://github.com/GrantCuster/garden).

I think I'm going to try a new blog concept (a continuation of [the thread-focused blog idea](/2024-06-05-20-27-27-Thread-focused-blog-idea/)), that combines what I do on [feed](https://feed.grantcuster.com).

Restarting is maybe a bit risky, since I think I have about three blog-like things I've made over the past few years, and this one I'm at least writing in. I resolved to keep writing in this one until the next one's real.

I'm hopeful the new one will get going though. I'd really like these kinds of posts to live next to lighter-weight screenshot + gif posts. I feel like I have good sense of how to keep the pieces relatively simple, and as long as I stick to well-trod ground for a lot of the pieces I can get decent AI generated code for a lot of it. Then I can hopefully splice and stitch that together into a lively little website.

So far I made an image uploader for s3, when some auto-resizing capabilities built-in. One of the downsides of when I used to have a static blog on github pages was how, over time, the images did come to dominate the repo size. I want to split out that chunk, wrap it up, and plug it in where I need it.

I (with ChatGPT assistance) got the uploader tool up on a password protected subdomain, being served through a droplet. Even did the nginx reverse proxy with SSL and feel like I pretty much understand how it's all working.

Basically want to put together a bunch of little web tool capabilities for myself and then start putting them together. Keeping the pieces big and blocky so they can be torn down or replaced or reconfigured.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-08-14-22-12-How-this-works</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-08-14-22-12-How-this-works</guid>
            <pubDate>Sat, 08 Jun 2024 14:22:12 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-07-09-01-29-Lenses]]></title>
            <description><![CDATA[# Lenses

Thinking about things that make you see the world around you differently: ARGs, birding, mushroom hunting, simulating physical phenomena in code, drawing.

I was listening to the [Egglant podcast interview with the Animal Well maker](https://eggplant.show/139-sticking-to-your-systems-with-animal-well) and they mentioned "secrets in plain sight" in the game and compared it to ARGs, where your street becomes something new/different to you because there is a geocache (or pokemon station there).

That made me think about [a tweet I saw yesterday from Jer Thorp](https://x.com/blprnt/status/1798698976495235307) about how birding helps you to see more details around you. Someone else mentioned taking up mushroom hunting had a similar effect.

I've thought about this when trying to simulate physical phenomena in code like water or raymarching, causes you to most closely observe the physical world.

A classic example of this is drawing, where one of the best instructions I got from the art teacher was that you had to look at something how it actually looks, not how you think it looks.

One of my favorite explorations of this idea is China Mieville's The City & the City, where two cities are overlaid on top of one another and living in one is a process of ignoring the other.

]]></description>
            <link>https://garden.grantcuster.com/2024-06-07-09-01-29-Lenses</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-07-09-01-29-Lenses</guid>
            <pubDate>Fri, 07 Jun 2024 09:01:29 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-05-20-27-27-Thread-focused-blog-idea]]></title>
            <description><![CDATA[# Thread-focused blog idea

What if I made a blog like this blog but also more like Twitter. Where I could make little posts, or I could also reply to old posts, bumping that 'thread' of posts to the top. But (like Twitter) it would show the update, with an indication and ability to click through to see the full thread.

That seems kind of cool? Like it'd be good for long-running thoughts I keep returning to and also for WIP posts...

Maybe structuring stuff in the code gets weird, but it's going to be small-scale so I think I could figure something out. I think I'd just do markdown files with like a link at the top, and use the filename for the date slug like I've been doing. I would still like titles I think. But maybe that's just otional.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-05-20-27-27-Thread-focused-blog-idea</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-05-20-27-27-Thread-focused-blog-idea</guid>
            <pubDate>Wed, 05 Jun 2024 20:27:27 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-05-20-07-01-Garden-plans]]></title>
            <description><![CDATA[# Garden plans

I would say this garden has been a relative success in terms of helping me write more. I kind of want to combine it with my feed, and maybe my homepage?

Feed would be the thing to do first I think. It's interesting trying to categorize the differences between posts. Garden is more writing based. Feed is structured around an image. Feed was designed to make sure I wasn't blocked trying to think of a title, that I could do more work-in-progress stuff. Maybe that's different now that garden is here.

Overall I want to keep things simple. But I also want to have some fun. I'm pretty confindent I can figure out ways to do what I want, and as long as I keep the file format relatively stable, markdown for garden and a common structure for feed, I think I can do it without really risking anything.

I started work on an image uploader, I think I want one that will just run locally and let me easily drop in images and get a public storage bucket address to link to. I kind of did that before for the writing app and it's worked well.

I guess one question is how to interleave garden and feed posts. Garden is static-based files right now, feed is a DB. I wonder if I would ever try and migrate feed to file based. There are quite a few posts in there, but if it's just the text it'd be pretty light, and I could build without too much trouble.

But what do I want it to feel like? I want it to feel alive. I want it to feel like the twitter feed of my profile but better. It could almost be its own mastodon feed I guess... But then I kind of like the idea of this standing on its own and then social being linked in.

It'd be cool to build your own threads too. Hmmm that kind of makes me want to build something like Twitter, but maybe friendlier to long text posts. Hmmm why not do that.

A thread focused blog... hmmm. Or would that just be overkill in terms of complexity. I wonder if I could tie that in with the garden metaphor in a way that doesn't feel too forced. Going to think about that one.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-05-20-07-01-Garden-plans</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-05-20-07-01-Garden-plans</guid>
            <pubDate>Wed, 05 Jun 2024 20:07:01 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-03-20-53-00-Gif-recording-workflow]]></title>
            <description><![CDATA[# Gif recording workflow

One of the best things I had going with my old linux set up was my gif recording workflow. I had a few helper scripts connected to dmenu (and then I think rofi and fzf), where I could record the screen and then convert into gif. It was still a bit janky. But it made it pretty quick to take work-in-progress clips and post to [my feed](https://feed.grantcuster.com).

Since I tore down my set-up and rebuilt on Ubuntu server install and Nix home-manager I haven't had a good gif set up. Tonight I think I got it fixed up though I'm sure I will continue to tinker.

The bulk of what makes it work is in [gif.sh](https://github.com/GrantCuster/nix-simple/blob/a8569ed3c3b25ba11066d449bdcdb0276f07856f/home/scripts/gif.sh). The majority of which is from [this comment by IntelligentPerson_ on Reddit](https://www.reddit.com/r/swaywm/comments/vr78q2/comment/l0atg5j/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1). Slightly modified to fit with keyboard shortcuts. It's even got nice little notifications.

The rest of what makes it work are in my [sway config](https://github.com/GrantCuster/nix-simple/blob/a8569ed3c3b25ba11066d449bdcdb0276f07856f/home/sway/config) and [status script](https://github.com/GrantCuster/nix-simple/blob/a8569ed3c3b25ba11066d449bdcdb0276f07856f/home/sway/status.sh). I even have a little recording indicator this time around. Something I never had in the last one.

I can't say I enjoy writing bash, but I'm starting to see more patterns, and have more of a base to reuse snippets from.

Here's an example gif:

![My sway setup featuring pokemon oblique strategies script](https://db-feed.s3.us-east-1.amazonaws.com/next-s3-uploads/4531ea8b-91d0-4d4b-8872-d1fd9a8cec84/2024-06-04_00-39-08.gif)
]]></description>
            <link>https://garden.grantcuster.com/2024-06-03-20-53-00-Gif-recording-workflow</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-03-20-53-00-Gif-recording-workflow</guid>
            <pubDate>Mon, 03 Jun 2024 20:53:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-06-02-14-52-50-Is-everything-a-prototype]]></title>
            <description><![CDATA[# Is everything a prototype?

## Prototypes should look like prototypes

Prototypes shouldn't look too nice. They should look like a work-in-progress. They should [show their joints](/2024-05-31-07-16-31-Carving-at-the-joints/).

This keeps discussion at the right level of abstraction. You want to be talking about the capability, not the color scheme.

It also invites others to extend and modify the prototype in their mind. It encourages iteration.

## But then...

I believe in and make these arguments. But then as I make the case, I start to question, what work shouldn't have these qualities?

Shouldn't we always invite understanding and potential modification?

Should everything be a prototype? Is everything a prototype?

I think this connects to that Steve Jobs quote about the empowerment of understanding everything in the built world was designed by someone, and you can do it too.

Qualities that might be in conflict with prototypey-ness are durability and safety. Important things!

But I think a lot of objects aspire to a feeling of 'completeness' that is uneccesary.  They want to conceal evidence of their construction. I want to show it.
]]></description>
            <link>https://garden.grantcuster.com/2024-06-02-14-52-50-Is-everything-a-prototype</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-06-02-14-52-50-Is-everything-a-prototype</guid>
            <pubDate>Sun, 02 Jun 2024 14:52:50 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-05-31-07-16-31-Carving-at-the-joints]]></title>
            <description><![CDATA[# Carving at the joints

I've always liked building systems where the joints are visible: legos, Minecraft. Lately I've been thinking about the process of learning how to find the joints in systems where they're less visible. Of 'carving at the joints'.

## The concept

Apparently Plato talked about "carving nature at the joints". I think my recent reference is the show Delicious in Dungeon, where they explain how to cut and prepare dungeon monsters into unlikely meals. The show has a lot of fun with the instructions reveal, where something that looks inedible is made appetizing through a precise set of steps.

Joints are opportunities for you to modify, to splice in or switch parts.

I've been thinking about this with electronics. Compared to something like woodworking, electronics seems harder to tell where the joints are. But as you try a few projects, and watch the YouTube DIY examples, you start to see. Almost like x-ray vision.

## Carving up computer hardware

For computer projects you can start with the components that are actually sold separately. Screen, computer, battery, keyboard and mouse. I've got a kind of portable cyberdeck going now with a Mini PC, a battery, a set of AR glasses and a keyboard. A laptop broken into it's parts and rearranged.

I've also partially disassembled some laptops, removing a broken display and instead routing it to a monitor for a 'headless' laptop. I've accidently disconnected the Wifi antenna and needed to add it back in -- a process which makes more concrete to me the wifi antenna as a piece of tech.

## Carving up AI

I like the idea of 'looking for the joints' when I approach any system. I've been thinking about this even with AI model experiments. Models are a weird set of capabilities, I want to find the right set of capabilities and splice them together with other software pieces to make the creative experiences I'm after.
]]></description>
            <link>https://garden.grantcuster.com/2024-05-31-07-16-31-Carving-at-the-joints</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-05-31-07-16-31-Carving-at-the-joints</guid>
            <pubDate>Fri, 31 May 2024 07:16:31 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-05-30-20-37-00]]></title>
            <description><![CDATA[# AI Image Map Brainstorm

Through work, I've been playing with the Gemini model's ability to return bounding boxes for an image. I think I want to try and use it to make some sort of image (sketch or photo) to website generator. Maybe leaning into skeuommorphism of like a photo of a physical desktop that contains all the links to my projects as items on the desk.

[A very sketchy proof of concept](https://feed.grantcuster.com/post/1717115954)

Here's a brainstorm:
I could do this for my homepage and make a new sketch each week. I would just want to ensure that it matched all the links. So the  builder could be a list of links and a place to add an image. You get the boxes back from the model and you have something tgo check if all of the links got placed and maybe if all of the boxes are past a certain size threshold.

This also connects to being a bridge between the physical and the digital world, similar to some of the dynamicland stuff, kind of. I could have digital links attached to objects I move around in the real world. I could also maybe connect this to the thermal printer.

Oh! One thing you could do is combine it with a webcam, have pieces of paper (or objects) that serve as links. Then baically have the computer monitor those boxes for a... gesture? or just a finger entering the space? Or hmm maybe just a major change -- use that as a proxy for clicking. Then you could build a physical interface...

But what does that help? Something that could help give you clarity of focus. Reduce the options to just the task at hand.

Physicallizing digital objects. Making them less slippery.

That connects to a more tools for thought. Something that would let you make something more like a node graph. You could write out a concept then tie it to a physical token, and then you could rearrange those physical tokens, and on the digital side it would... what? Mirror the arrangement of the phsyical tokens? (Possible but necessarily that interesting?) What do I actually want to do? I want to be able to pack a complex idea into a movable container, but then I also want to be able to separate out pieces of that idea (that's why too rigid a structure doesn't work).

There's also the arrows angle, doing something when you draw arrows...
]]></description>
            <link>https://garden.grantcuster.com/2024-05-30-20-37-00</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-05-30-20-37-00</guid>
            <pubDate>Thu, 30 May 2024 20:37:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-05-29-22-11-34-I-got-the-bluetooth-paired]]></title>
            <description><![CDATA[# I got the bluetooth paired

I'm not sure if it was from reflashing the keyboard firmware or resetting the computer software but it's working now. It is nice. I'm not sure what lesson to take from this.

Am I writing too many posts now? Still getting a feel for this thing. It is fun the ways different writing environments seep into the writing.
]]></description>
            <link>https://garden.grantcuster.com/2024-05-29-22-11-34-I-got-the-bluetooth-paired</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-05-29-22-11-34-I-got-the-bluetooth-paired</guid>
            <pubDate>Wed, 29 May 2024 22:11:34 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-05-29-20-12-07-Obsessive-bluetooth-pairing]]></title>
            <description><![CDATA[# Obsessive bluetooth pairing

I'm having trouble pairing my wireless keyboard with my mini PC. I keep trying new things and searching the proglem in different combos.

My current theory is maybe the bluetooth driver isn't up to date enough? But I think it actually may be.

The haunting thing is maybe it's just a bad state issue, and if I reset the right thing (firmware on the keyboard, bluetooth status on the keyboard) it will be all fixed up.

I should leave it. I have other things to do. It can sit. I can use the keyboard in wired mode with the mini PC and it works fine.

But every time I step away I think "well maybe..." and I have a strong urge to try something slightly different.

I think this general pattern has probably served me well in many computer and life things and badly in others. I'm going to do some chores now, but I also want to reflash the firmware.
]]></description>
            <link>https://garden.grantcuster.com/2024-05-29-20-12-07-Obsessive-bluetooth-pairing</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-05-29-20-12-07-Obsessive-bluetooth-pairing</guid>
            <pubDate>Wed, 29 May 2024 20:12:07 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-05-28-07-31-35-Writing-on-the-couch]]></title>
            <description><![CDATA[# Writing on the couch

Trying to keep up the habit of quick writing in here. Now morning writing from the couch, the dog beside me, the baby exploring a cardboard box. A nice spring-like morning.

Did some organizing syncing the setups yesterday. Still liking the Ubuntu + Nix configuration. Getting better and faster about keeping it up to date across the laptop and the mini PC. The server's a bit behind.

For the glassese set-up I got audio and bluetooth working yesterday, so I think everything I wanted to do with that thing is proved out. I ordered a cross-body bag that I hope will fit everything well. Porch use has been good so far, interested in trying a commute with it. The battery pack has been working well, I'm surprised how much having an exact percent readout, rather than something like 4 lights, helps me feel secure about power draw.

I want to get clipboard history sorted today. Also want to make the nice gif recording set up with a notification in the top bar.
]]></description>
            <link>https://garden.grantcuster.com/2024-05-28-07-31-35-Writing-on-the-couch</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-05-28-07-31-35-Writing-on-the-couch</guid>
            <pubDate>Tue, 28 May 2024 07:31:35 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-05-26-20-43-14-Writing-on-the-porch]]></title>
            <description><![CDATA[# Writing on the porch

Hopefully the first of many posts written while sitting on the porch. This is the first one I've got set up with the AR glasses.

It was cool earlier when the light meant I could see more of the background, so it felt like a floating transparent screen. Now it's darker and the screen is less transparent.

I'll do a bigger post on the set up later, but I can list out briefly what I have going:

- Mele fanless mini PC
- Charging brick that outputs 45 watts
- Nreal AR Glasses
- Technikable keyboard
- Software running Sway and Neovim to write in

The battery / glasses / mini PC combo was recommended on Reddit and I'm happy it's all working together so well. The keyboard fits in well, though I still want to get mouse keys working. I'm excited for writing in this, should also be good for entertainment, even, I think, on the subway. I want to get or make a good cross-body bag to use with it.
]]></description>
            <link>https://garden.grantcuster.com/2024-05-26-20-43-14-Writing-on-the-porch</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-05-26-20-43-14-Writing-on-the-porch</guid>
            <pubDate>Sun, 26 May 2024 20:43:14 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-05-26-12-21-28-Today-in-computer-projects]]></title>
            <description><![CDATA[# Today in computer projects

This blog!: I want to get preview images working. I'm trying to generate them using cairo, so I can do it as part of the build script. I would say I've proven out the basic concept, now need to decide on some text positioning and hook it all together for the first run.

Mini PC: I'm working on getting a mini PC set up with the my current go-to Ubuntu server + Nix home-manager config. Eventually I want to try and make it a wearable deck to go with my AR glasses. Will write about that more later.
]]></description>
            <link>https://garden.grantcuster.com/2024-05-26-12-21-28-Today-in-computer-projects</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-05-26-12-21-28-Today-in-computer-projects</guid>
            <pubDate>Sun, 26 May 2024 12:21:28 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-05-25-16-28-31-Posting-from-a-mac]]></title>
            <description><![CDATA[# Posting from a mac

I want to make sure I can write from anywhere, including my work mac, which I'm writing on now. I had to update bash on the mac (I did it using Nix home manager). Otherwise things seem to be working well.

Edit: also had to replace a `grep P` with a `sed`.
]]></description>
            <link>https://garden.grantcuster.com/2024-05-25-16-28-31-Posting-from-a-mac</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-05-25-16-28-31-Posting-from-a-mac</guid>
            <pubDate>Sat, 25 May 2024 16:28:31 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-05-25-15-17-44-Mic-check]]></title>
            <description><![CDATA[# Mic check

Today I made this blog / garden. I want it to be a place where I can write quickly and with focus.

Right now it's a bunch of bash scripts that put together some templates. I wanted to know exactly what was going on so that I'll be able to modify it in the future. Though to be transparent the bash scripts were AI assisted. Still, I think I understand all the steps.

I want to keep things short.
]]></description>
            <link>https://garden.grantcuster.com/2024-05-25-15-17-44-Mic-check</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-05-25-15-17-44-Mic-check</guid>
            <pubDate>Sat, 25 May 2024 15:17:44 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2024-04-15-20-35-17-Lowstakes-physical-materials]]></title>
            <description><![CDATA[# Low-stakes physical materials

Compared to digital constructions, there is a higher cost to starting over with physical work. But there are materials you can choose where it's much lower-stakes, and I've been having fun assembling a set of them:

- Cardboard: a standby of prototyping, easily cut, surprisingly sturdy, readily available from shipping boxes, if cardboard wasn't so ubiquitous it would be a lot more appreciated. I at least do a first version on most things in cardboard, and often it lasts long enough that it stays cardboard. Goes great with hot glue but usually I just use tape.
- Masking tape: good for cardboard, good for taping up inspiration, good for writing on with sharpie.
- Air-drying clay: this a new one. I've made some keyboard palm rests, a new housing for a wireless phone charger, and a holder for a set of cards. Not as nice looking as fired clay, it's still wonderful to be able to sit down with an idea and rough it out in 15 minutes. Clay gives you shapes cardboard and wood aren't friendly too.
- Velco - I've got rolls of velcro with adhesive on one side. Makes for easy keyboard and phone mounts.
- Monitor poles and arms: I've rearranged monitors so many times. Poles and the arms that go on them are generally modular and you can figure out ways to mount non-monitor things as well.  Most recently I used hose clamps to mount plywood panels flat against the poles. You can also get them in suprisingly tall sizes. If I were running a co-working space I'd make walls out of them.
- Plywood: wood requires a lot more tools and set-up. I've got a decent set now, but I still try and set some constraints to not get bogged down in perfectionism. I got four sheets of 3/4" birch from home depot a while ago, and they lasted me many years. Lately I've taken to breaking down old under-used projects and reusing the wood, feels surprisingly satisfying. I try and keep it simple. Cut into rectangular shapes, mostly hold together with wood glue -- screws when needed.
- Hinges and connector plates: with the plywood I've been venturing into hinges. Had a real "there are no rules" realization moment with the fact that you can just go buy hinges and make whatever kind of folding hinge-y thing you want. Starting to notice all the hinges around me.
- Fabric and sewing: I've done some bag modifications, often using the velcro. I think there's a lot more I could do here. Fabric, like clay, is a kind of differently shaped thing than cardboard and wood, having better capabilities there will probably open up some new possibilities.
- Legs: having my adjustable standing desk legs separate from the desk top has led to a few rounds of experimentation. You can just generally take and reattach the legs from lots of things.
- Plexiglass: I've mixed in a bit of plexi with some of my wood work. It's a little pricier, especially as you get thicker, so I only have thin stuff which in turn feels a little delicate when you stretch it over too great an area. Haven't quite got the feel for when to reach for it but I'll get there someday.
- Screens and keyboards: this is stretching a bit, but having lots of custom-built keyboards and random monitors around does encourage more weird computer constructions, and as long as you don't break them, they can be repurposed again and again.
]]></description>
            <link>https://garden.grantcuster.com/2024-04-15-20-35-17-Lowstakes-physical-materials</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2024-04-15-20-35-17-Lowstakes-physical-materials</guid>
            <pubDate>Mon, 15 Apr 2024 20:35:17 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2023-10-09-10-38-38-What-did-pokemon-do]]></title>
            <description><![CDATA[# What did Pokemon do?

Pokemon took the combat formula from traditional turn-based RPGs and turned it inside-out.

In traditional RPGs you have a few characters who learn more and more attacks and spells.

In Pokemon you capture more and more characters (pokemon) who can only have four moves each.

Instead of choosing from a big list of attacks and spells, you choose from a list of Pokemon and then a small list of moves.

I'm interested in the shift from "few characters, many moves" to "many characters, few moves each" as a design move that could be useful to apply to interfaces.

Many traditional RPGs (I've been playing Chrono Trigger) are recognized as some of the best videogames ever made, but they don't rival pokemon in popularity.

How much can you put that difference down to the shift in combat? There are definitely some other, related aspects:

- Capturing means regular battles are now opportunities to create more characters. Regular battles are the grindiest aspects of RPGs so this seems like a big deal.
- Evolution mechanics tap into people's interest in digital pets.
]]></description>
            <link>https://garden.grantcuster.com/2023-10-09-10-38-38-What-did-pokemon-do</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2023-10-09-10-38-38-What-did-pokemon-do</guid>
            <pubDate>Mon, 09 Oct 2023 10:38:38 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2022-09-04-09-58-24-Expressive-tools-for-thought]]></title>
            <description><![CDATA[# Expressive tools for thought

I'm excited for all the energy around tools for thought. Some notes on my current thoughts (probably to be revised in future entries).

## Blocks as the basic unit

Blocks as a basic, contained, and usually addressable unit feels to me "right". You can see it in Notion and Roam and many others.

I think the collapsible block (block that can have children) is also an essential piece as well. This is in Roam and Logseq (and Workflowy!). It's not in Notion or Obsidian as the default, and I miss it there.

The nested block structure makes a tree. I think one of the best parts of the tree structure is how it lends itself to restructuring. Move the top-level item and you move the children along with it. You can organize your writing at the top-level, or you can drill down into an item and reorganize within that. That feels right to me. It's notecard reorg, but even more flexible.

Trees are exclusively hiererchical and sometime relationships are not. This is where hyperlinks come in. And Roam did a great job of showing the possibilities here, with instant create and backlinks. With links + nested blocks you can have hierchical structure while also making these associative jumps. This seems like the right mix. I'm not sure we've cracked the best way to present it yet, I still get stressed trying to make sure I figure out the right link/category naming conventions.

## Blocks as individually addressable

Connected to this is the possibility of addressing indivdual blocks, whether creating links or something more like transclusion. Nested blocks plus addressing blocks seems to me the closest we've got to the promis of "what computers are good at" being marshalled for our own ends.

### Naming

Technically, the individually addressable blocks are interesting. The way I know to do it is to assign each one a uuid, which can be created in isolation with a strong guarantee that it won't collide with other individually created ids. This is cool. It helps with mutliplayer, it helps with simplified/contained creation.

With uuids we can have guaranteed unique ideas for all your bullet points.

But we've still got a naming issue. How do you refer to a block when you link to it? The uuid does not exactly stick in the mind. But it's also too much of a burden to ask you to name or tag each one of your bullet pointss. Use a section of the text maybe? Or bring up some fuzzy search interface that makes the linking easy -- still it's difficult to zero in on what in that bullet is useful important (say you wanted a zoomed out structural view, for example). Maybe machine learning bassed summarization helps here.

## Expressiveness

My current hot-take is that expressing yourself is intertwined with thinking, and none of the tools so far have paid enough attention to this aspect. I'm fascinated by the aesthetic college class notes culture around Notion, but I also find Notion's rules so limiting in expressiveness. Especially compared to the "older web" where I most often think of tumblr and how much room to make a genuinely unique blog structure there was (combined, interestingly, with the guardrails of the feed).

### Website-builders

I think expressivity has been under-addresssed. I also think html and CSS have advanced to the point where we can give users a subset
]]></description>
            <link>https://garden.grantcuster.com/2022-09-04-09-58-24-Expressive-tools-for-thought</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2022-09-04-09-58-24-Expressive-tools-for-thought</guid>
            <pubDate>Sun, 04 Sep 2022 09:58:24 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2020-09-12-12-09-04-The-benefits-of-limitations-in-application-launchers]]></title>
            <description><![CDATA[# The benefits of limitations in application launchers

In my Linux set-up, I use dmenu as an application launcher. dmenu is basically autocomplete for applications and scripts. In many ways, it's not so different from launching things using Spotlight on a Mac.

![Opening 750words.com with dmenu and a launcher script.](https://grant-uploader.s3.amazonaws.com/2024-08-21-20-15-51.gif)

Since I started using it, dmenu has been a convenient way to launch apps. But I've only recently started to realize some of the interesting things it makes possible. A lot of the possibilities come down to the limitations of the interface, and how agnostic it is about what it launches.

Lately, I've started putting scripts to run apps, or a combination of apps, or a combination of apps and websites and terminals, in my `.local/bin` folder where they are exposed through dmenu. Some of the things I've added:

- `planner` launches my calendar, personal email, and work email all in different firefox windows.
- `750words` launches https://750words.com/ in a new firefox window.
- `blog` launches a terminal set to my blog directory, a terminal window running `npm run dev` to run the local versions, and a firefox window open to localhost.
- `record` starts a screen recording. `gif_2x` converts the last screen recording into a GIF running at 2x speed. When it's done it opens a window showing the GIF and listing the file size.

Some of these are rather involved scripts, some are very simple. What I've been surprised by is how different even the simple ones feel when launched from the application launcher. Something like `750words` is given its own space and weight as an activity, promoted from being a website I visit among other websites. It also makes my intention going in very clear, if I open `750words` my intention is to do it, versus getting lost browsing (even though, functionally, all it is doing is opening `750words` in a web browser). 

I used to go after the same effect on macOS, especially for web applications. There were several programs that would let you run a website within its own application container. That meant you could launch it from Spotlight. There were always rough edges that made clear it was a bit of a hack though. The difference between the wrapped web apps and the true apps was noticeable. This is partly because apps on macOS are expected to be polished, with their own nice-looking icon. 

**The lesson I'm interested in with dmenu, is how the limitations (an application is only a name among names) make it much easier and more satisfying to add user-configured applications and scripts.** I am surprised by how different it feels to have my scripts sit right alongside the other applications. As with my experience of lots of Linux-related stuff, it is a feeling of empowerment. It feels like a level of customization above what I'm used to, computer as tool I am control of, rather than something I'm wrestling with.

## Content limitations in web previews

I've noticed a similar effect in making websites. Often you end up making a website that you want to display a preview of on another website. I do this on [Constraint Systems](https://constraint.systems) and for the [Cloudera Fast Forward prototypes](https://blog.fastforwardlabs.com/prototypes). Through experience, I've learned you want to make the number of assets needed for the list preview as few as possible. In the case of Constraint Systems, it is a tile, a description, a preview image, and a preview GIF. It is tempting to require more elements (like several preview images) to provide a fancier preview. Whatever you require, however, you're going to have to provide for every link going forward. A surprising amount of friction can come from needing to create preview assets (and also the deploy process for those assets). Enough friction to cause you to make fewer tools or blog posts because you're dreading that part of the process.

Website meta tags have been an interesting development in relation to this. Used for creating previews on shared Twitter and Facebook links, the main meta tags are limited enough (title, description, preview image) that they're worth doing, and now that they're being regularly done across sites, even more services can dependably use them to unfurl links.
]]></description>
            <link>https://garden.grantcuster.com/2020-09-12-12-09-04-The-benefits-of-limitations-in-application-launchers</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2020-09-12-12-09-04-The-benefits-of-limitations-in-application-launchers</guid>
            <pubDate>Sat, 12 Sep 2020 12:09:04 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2020-09-01-11-50-28-Sift-release-notes]]></title>
            <description><![CDATA[# Sift: release notes

![](https://grant-uploader.s3.amazonaws.com/2024-08-21-20-10-23.gif)

[Sift](https://sift.constraint.systems) is an experimental image editor that slices an image into layers. You can offset the layers to produce interference patterns and pseudo-3D effects. It uses an additive blending mode and pixel-based light splitting algorithm.

## Origins

I started planning Sift while standing in the ocean, thinking about waves and how to use a wave effect on the pixels of an image. I've gotten used to thinking of images as a grid of pixels, and I've done some experiments using HTML canvas and javascript to move, or even flow, pixels around. I started trying to imagine how pixels could cycle "below the surface" and then pop up on top.

For a wave effect I needed pixel depth. I needed to figure out a way to transform an array of pixels from 2D into 3D. I thought about RGB values. Coud I use the color value as the third dimension by using it to make a pixel stack? What if for a pixel with a red value of 100 I stacked 100 red pixels?

I ended up using the pixel stack idea (and cutting the wave idea, for now), but I had to get the right blending mode and slicing algorithm to get things working.

## Slicing colors

One of the issues with making pixel-specific stacks was that a pixel doesn't have just one color value, it has three (red, green, and blue). I decided to put the "brightest combo" at the top. So for an RGB value of `[10,16,24]` I would start the stack with 10 `[1,1,1]` pixels. Then `16 - 10 = 6` for 6 `[0,1,1]` pixels, and, finally, `24 - 16 = 8` for 8 `[0,0,1]` pixels. This means the white-ish pixels are on top, and then, as those are finished, you see a kind of exhaust trail of color.

(For performance reasons, the finished app bins values according to the number of layers. So instead of 192 stacked `[1,1,1]` pixels for a `[192,192,192]` value, a 16 layer edit in Sift bins `192 / 16 = 12`, for 12 `[16,16,16]` pixels.)

## Blend mode

![Overlapping red, green, and blue squares in additive blend mode.](https://grant-uploader.s3.amazonaws.com/2024-08-21-20-10-23.gif)

The color slicing only works with right blend mode, where each layer's RGB value is added together. For canvas, the blend setting is called `globalCompositeOperation` and the value for additive blending is `lighter`.

Additive blending, while not what I'm used to working with for computers, is actually how our eyes perceive light. I vaguely knew this, but it's been fun to play with it in the app and get a real feel for the consequences.

## Layers and offsets

Originally, I thought I'd build this app in Three.js, where you could rotate around the pixel (actually voxel) stacks in 3D. I thought maybe I could get the perspective such that it appeared a whole image at the start, but as you zoomed in you could see cracks between. I'm still not totally sure if the math for that could be worked out. But I quickly ran into performance issues from trying to render even binned values for every pixel in an image, so I switched over to HTML canvas. (I'm sure there are ways to do this in Three.js, possibly utilizing shaders? If you have ideas let me know.)

I knew performance might be a struggle in canvas as well, but I had a plan. I know canvas can redraw image files (with `drawImage()`) quickly. On image load I split the image into a set number of layers (16 by default), doing all the bin calculations. The render function (peformed whenever the x and y offsets are changed) then just draws those layer images on top of one another and the blend mode takes care of the rest.

## The result

Sometimes I have specific goals for [Constraint Systems](https://constraint.systems) projects, other times I just follow an effect to its end. Other than the "stack" idea, I didn't really have a goal for what Sift should make images into. I was pleasantly surprised by the early results, right after I flipped the blend mode switch. For certain images, it produces a pseudo-3d effect. [Ezekial suggested it's like an aerogel](https://twitter.com/the_ezekiel/status/1299095952339410945). It is also kind of like badly calibrated color separation in a TV (but different because of how it is stacked). Other images you can get kind of an otherwordly thing. I watched _Twin Peaks: The Return_ recently and it reminded of me some of the face distortions from that.

## Future experiments

I'm definitely intrigued by the possibilities of additive blending. Partly due to [Tyler's suggestion](https://twitter.com/tylerangert/status/1299163673424879617), I want to try layering video frames on top of each other using a similar process.

## Larger goals

One of the goals of the Constraint Systems projects is to get really used to thinking of images as a collection of pixels, and work "with the grain" of how computers store images. I think the stacking and layer ideas are good signs that that part of the project is working. My intuition for what might produce interesting image results has gotten better. The mix of having a good idea of what I wanted but not being sure of the final effect is a fun one -- it feels like a collaboration with the computer.
]]></description>
            <link>https://garden.grantcuster.com/2020-09-01-11-50-28-Sift-release-notes</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2020-09-01-11-50-28-Sift-release-notes</guid>
            <pubDate>Tue, 01 Sep 2020 11:50:28 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2020-08-20-08-32-35-Fantasy-consoles-and-framing]]></title>
            <description><![CDATA[# Fantasy consoles and framing

I've been thinking a lot about [Joseph White's talk on his motivations for making the PICO-8 fantasy console](https://www.lexaloffle.com/bbs/?tid=31634). There's so much in the talk that resonates with what I've been thinking about for [Constraint Systems](https://constraint.systems): about how carefully selected constraints change the feel of working, making it feel more focused, and even cozy.

Since viewing the talk I've been thinking a lot about how he frames PICO-8 with the idea of a fantasy console and cartridges, and what I could do for framing Constraint Systems. I've toyed with the idea of making the Constraint Systems homepage into a simulation of a fantasy operating system with each experiment as an application. Part of the feeling I want to capture is going to the middle school computer lab in the mid 90s and trying out the strange collection of software the school had preloaded (even though the variety of the internet is great, there is something comforting and cozy in the idea of a finite number of programs to explore).

I had been thinking of the operating system metaphor as a fun, possibly attention-attracting, thing, that I should get around to sometime. After viewing White's talk, however, I think it's something I should prioritize. Framing Constraint Systems as a fantasy computer/operating system could (done well) communicate my vision of the project, and communicate it not in a long text somebody has to read, but as a general vibe. In the best case, they would "get" the project just by looking at the homepage. This is what "branding" is, I suppose, it just feels more tied to the core of the project here than I'm used to thinking of it.

Extensions of the idea:
- The simplest version is just presenting the Constraint System experiments as different apps on a fantasy operating system. I could also try and make them behave as apps. Possibly using iframes and a tiling window management system. A further step (that I've always wanted to do) would be to let you pipe the output of one application into another.
- Picking up on the middle school computer lab vibe, I wonder if Constraint Systems could someday be a physical computer lab, where computers limited to only CS software are available free for anyone to use, and I administer the lab and get to see what people make and can adjust or make new applications based on what people are doing with it. (I think this is at least a good idea for an installation or area at a hackerspace.)
]]></description>
            <link>https://garden.grantcuster.com/2020-08-20-08-32-35-Fantasy-consoles-and-framing</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2020-08-20-08-32-35-Fantasy-consoles-and-framing</guid>
            <pubDate>Thu, 20 Aug 2020 08:32:35 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2020-07-30-10-34-10-Automadraw-release-notes]]></title>
            <description><![CDATA[# Automadraw: release notes

![](https://grant-uploader.s3.amazonaws.com/2024-08-21-20-00-13.gif)

[Automadraw](https://automadraw.constraint.systems) is a new experimental app I made for my [Constraint Systems](https://constraint.systems) project. It lets you draw and evolve your drawing using cellular automata using two keyboard controlled cursors.

## What is it for

I think there are two main uses for Automadraw:
1. Get more familiar with the cellular automata (Conway's Game of Life and Langton's Ant) that it runs. You can quickly experiment with lots of different patterns.
2. Draw something collaboratively with the automata. The interaction design aims to make working with the automata intuitive. These design techniques (two cursors, keyboard controls) could be applied to a wide range of creative apps.

## Two cursors

I had originally planned to use just one cursor, and have it shift between draw mode and "act" (run automata) mode. As I experimented I found that usually for act mode I wanted to cover a large area and draw mode a smaller one. Having to resize when switching between modes ruined the flow, so I split the cursors up. 

Splitting them up opened up some new possibilities. I realized I could set it up so that I could use each cursor's actions (draw or act, respectively) regardless of which one was in focus. This set up a couple of interactions I really liked:

![Sweeping: draw some lines then use a long, narrow act cursor to sweep over the lines, running Game of Life over each sweep step. This usually produces intricate symmetrical designs that really feel like they're evolving through each sweep.](https://grant-uploader.s3.amazonaws.com/2024-08-21-20-01-35.gif)

![Active environment: resize the act cursor over a large area, use the draw cursor and have it move in and out of the act area as the automata is run. The act area becomes an environment where different rules apply. It feels like a physics or chemistry simulation.](https://grant-uploader.s3.amazonaws.com/2024-08-21-20-02-29.gif)

This set-up is uniquely suited to keyboard cursor controls, where each cursor's position is fully visible and fully predictable (versus a touch interface where you would have to use multiple fingers and the fingers themselves would obscure your view of the changes taking place). I use Vim-like keyboard controls because I honestly prefer them. My suspicion is that they may enable modes of interaction other methods do not. I was happy to find an interaction that fit them so well. I'm looking forward to seeing how even more multiple cursors feel in future experiments.

[Stamp](https://stamp.constraint.systems) is a different example of the possibilities of multiple cursors: two cursors across two canvases.

![](https://grant-uploader.s3.amazonaws.com/2024-08-21-20-03-12.gif)

## Keyboard events

Part of the reason the two cursor interaction is interesting is because of an accident of keyboard event handling. A lot of the Constraint System experiments let you hold down multiple keys. This is tricky to handle in Javascript for everything except modifier keys. The main issue is that if you're holding down one key, and start holding an additional one, the new one will take over the `keyDown` event. The solution is to make a keymap object, store each key on `keyDown` and remove it on on `keyUp`. You then use the keymap object for the source of truth about what is pressed on each `keyDown` event.

This technique mostly just works, but there turns out to be an issue, arguably a bug, that makes things like the "sweep" technique I discussed above possible. If you are pressing one key, add another key, then let up on the second key, `keyDown` events stop firing. For Automadraw, this behavior enables this interaction:

- Hold down 'a' to run automata, press a direction key to move the act cursor. When you let up on the direction key the automata will pause running... until you press a direction key again. Using this technique you can run "sweeps", moving the act cursor across a set of pixels, automatically running the automata once each step.

This interaction was a happy accident, and I'm looking forward to thinking about how to expand and support it more in future experiments.

## Limitations and future possiblities

I had been wanting to experiment with cellular automata and a drawing app for a long time. For this experiment, I needed to really scope things down in order to get started. I restricted the drawing app colors to 1-bit (on or off). This usefully limited the number of cellular automata I could use and the number of interactions I needed to support. I also made the app 'pixels' large, at 16 actual pixels. This makes drawing quick and the automata actions more legible, but also restricts the fidelity of the final image. Someday I would like to build a cellular automata app more focused on image editing, where you could evolve parts of an image at a higher fidelity. That would also involve using automata that use color information, there are some interesting examples of those in [this CA Lab demo video](https://www.youtube.com/watch?v=lyZUzakG3bE).

## Code

The code for Automadraw is [avaliable on github](https://github.com/constraint-systems/automadraw).

### Slowly recreating React

I built the early Constraint Systems experiments using React, but have moved off of it to vanilla Javascript for the most recent ones. I do find myself recreating a lot of the set-up of React. I've found out firsthand that a lot of the React boilerplate I questioned is in there to work around the constraints of Javascript itself. I may switch back to React sometime, but right now I'm still enjoying experimenting on my own. It is also true that a lot of the benefits of React don't mesh well with HTML canvas, which is where most of the action for this app takes place.

### ES6 modules

This was the first project where I used ES6 modules. It was nice to be able to organize the code into sections like `keyboard` and `state`. I'll continue to use them and refine my organization going forward. Maybe someday I'll have a true base starter kit I can reuse across projects.

### Canvas compositing

One switch I've made that I've been very happy with, is moving from rendering multiple canvas DOM elements on top of eachother, to placing only one canvas on the dom and compositing the different layers (in this case: `cursor`, `grid`, `art`) on to the DOM layer for each render. My rendering code is a little knotty, but it still feels a lot cleaner than stacking the canvases in the DOM.


]]></description>
            <link>https://garden.grantcuster.com/2020-07-30-10-34-10-Automadraw-release-notes</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2020-07-30-10-34-10-Automadraw-release-notes</guid>
            <pubDate>Thu, 30 Jul 2020 10:34:10 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2020-07-28-21-37-28-Bushido-Blade-2-a-design-appreciationi]]></title>
            <description><![CDATA[# Bushido Blade 2: a design appreciationi

Bushido Blade 2 was a Playstation game I played a lot in high school. It was a fighting game with swords, and its main hook was that instead of health bars, damage was based on where you struck your opponent. You could injure limbs or finish the an opponent with one strike if you hit the right spot.

![A kill in Bushido Blade 2](https://grant-uploader.s3.amazonaws.com/2024-06-19-21-38-28-800.jpg)

Design-wise, Bushido Blade rethought the premise of a fighting game from first principles. I love what this approach allowed them to do in terms of immersion: during a fight, nothing is visible on the screen except the two characters. 

This is what I want to do when I design something: communicate everything through the core action. Design things so well that you don't need to bring in health bars and labels.

Bushido Blade 2, like most of the games I played, was well-reviewed but never really that popular. I remember it having a pretty good story-mode with fun voice acting. 
]]></description>
            <link>https://garden.grantcuster.com/2020-07-28-21-37-28-Bushido-Blade-2-a-design-appreciationi</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2020-07-28-21-37-28-Bushido-Blade-2-a-design-appreciationi</guid>
            <pubDate>Tue, 28 Jul 2020 21:37:28 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2020-07-12-21-24-56-Swapping-color-schemes-across-all-terminals-and-Vim-with-Pywal-and-Base16]]></title>
            <description><![CDATA[# Swapping color schemes across all terminals and Vim with Pywal and Base16

![Switching between light and dark colorschemes in all terminals using a hotkey.](https://grant-uploader.s3.amazonaws.com/2024-06-19-21-34-47.gif) 

I recently got instant light and dark color scheme toggle working for all open terminals, including those running Vim. I used a combination of techniques from [Pywal](https://github.com/dylanaraps/pywal) and [Base16 shell](https://github.com/chriskempson/base16-shell), and learned some things about scripting in Linux and escape sequences along the way.

## Pywal

[Pywal](https://github.com/dylanaraps/pywal) is a package for switching color schemes system wide. Mostly it is known for generating those color schemes from images, but it also comes bundled with a bunch of predefined themes. I wanted to use it to switch between [gruvbox](https://github.com/morhetz/gruvbox) light and dark themes.

Pywal can change the color schemes for all open terminals automatically. It can also switch colors for several other Linux applications.

### How Pywal works

This is what the gruvbox dark theme looks like in Pywal's colorschemes directory:

```
# Pywal gruvbox colorscheme
{
  "special": {
    "background": "#282828",
    "foreground": "#a89984",
    "cursor": "#ebdbb2"
  },
  "colors": {
    "color0": "#282828",
    "color1": "#cc241d",
    "color2": "#d79921",
    "color3": "#b58900",
    "color4": "#458588",
    "color5": "#b16286",
    "color6": "#689d6a",
    "color7": "#a89984",
    "color8": "#928374",
    "color9": "#cc241d",
    "color10": "#d79921",
    "color11": "#b58900",
    "color12": "#458588",
    "color13": "#b16286",
    "color14": "#689d6a",
    "color15": "#a89984"
  }
}
```
A JSON file declaring each color. OK, but how do those colors get communicated to the applications? The [customization](https://github.com/dylanaraps/pywal/wiki/Customization) instructions mention `~/.cache/wal` a lot, so let's see what's in there:

```
# ls ~/.cache/wal
colors                      colors-putty.reg         colors-tty.sh        colors.Xresources
colors.css                  colors-rofi-dark.rasi    colors-wal-dmenu.h   colors.yml
colors.hs                   colors-rofi-light.rasi   colors-wal-dwm.h     sequences
colors.json                 colors.scss              colors-wal-st.h      wal
colors-kitty.conf           colors.sh                colors-wal-tabbed.h
colors-konsole.colorscheme  colors-speedcrunch.json  colors-wal.vim
colors-oomox                colors-sway              colors-waybar.css
```

Ah! It's using the JSON color schemes to generate application specific color scheme files. This is  a great example of figuring out which level of abstraction to intervene at: Pywal defines a standard color scheme spec and uses application specific templates to generate files from it. If anyone wants to add a new color scheme or application template the procedure for doing so is clear and self-contained.

### Live reload and escape sequences

To change color schemes in most applications, Pywal builds the color config file and sends a message to the application to reload. For terminals, it does something different. It uses [ANSI escape codes](https://en.wikipedia.org/wiki/ANSI_escape_code), invisible character sequences that give a terminal color and formatting instructions, to instantly swap out the colors.

You can see how this works in Pywal's [sequences.py](https://github.com/dylanaraps/pywal/blob/master/pywal/sequences.py). The conversion from the JSON hex color to the terminal readable escape sequence is here: 

```
# from pywal/sequences.py
def set_color(index, color):
    """Convert a hex color to a text color sequence."""
    if OS == "Darwin" and index < 20:
        return "\033]P%1x%s\033\\" % (index, color.strip("#"))

    return "\033]4;%s;%s\033\\" % (index, color)
```

Escape sequences, which I've only seen otherwise in terminal prompt customizations, are not easy to parse or write for a human, but as part of a script they're a powerful way to achieve instant terminal color palette swaps. I don't think anyone would design an API featuring anything like escape sequences today, but in this case they make for a much smoother experience than a "change config and reload" cycle.

Now let's look at how the escape sequences get sent to the terminal:

```
# from pywal/sequences.py
def send(colors, cache_dir=CACHE_DIR, to_send=True, vte_fix=False):
    """Send colors to all open terminals."""
    if OS == "Darwin":
        tty_pattern = "/dev/ttys00[0-9]*"

    else:
        tty_pattern = "/dev/pts/[0-9]*"

    sequences = create_sequences(colors, vte_fix)

    # Writing to "/dev/pts/[0-9] lets you send data to open terminals.
    if to_send:
        for term in glob.glob(tty_pattern):
        util.save_file(sequences, term)

    util.save_file(sequences, os.path.join(cache_dir, "sequences"))
    logging.info("Set terminal colors.")
```

This shows the power of Unix's "everything is a file" approach. The script locates the file for each open terminal and writes the sequences directly to it (same as you would write to a text file). And it just works. 

### Vim issues

Pywal worked beautifully for me except for Vim. It may not be an issue depending on how your Vim and terminal color schemes are configured, but in my case to get the proper color scheme I needed to not only swap the terminal colors but also toggle the `background` setting in Vim between `light` and `dark`. I eventually got this working using `xdotool` to trigger a toggle hotkey in Vim, but it was not nearly as clean a process as the main `write directly to terminal` Pywal approach. So I went hunting for other solutions.

## Base16

[Base16](https://github.com/chriskempson/base16) is a standardized format for creating 16-color terminal color schemes. Those color schemes can then be combined with templates to produce color configurations for a wide range of applications. [Base16 shell](https://github.com/chriskempson/base16-shell) is a set of scripts that converts those color schemes into escape sequences to be applied to terminals.

The main draw for me for Base16 was that their [Vim package](https://github.com/chriskempson/base16-vim) lets you set a base Vim color scheme that works wonderfully with any Base16 terminal color scheme, no `background` setting change needed. (Pywal does also have a version of this, but I was much less impressed with the base Pywal Vim color scheme.)

### Applying Base16 to all open terminals

Base16 shell, unlike Pywal, only applies the new color scheme to your current terminal. This set-up has its own interesting possibilities (different color schemes for terminals where you `ssh`ed; random color scheme for each new terminal) but I wanted the color scheme to be applied globally. So I frankensteined a bit of Pywal into the Base16 shell script:

```
# Modified Base16 shell script
...
terms=`ls /dev/pts/[0-9]*`
terms="${terms} $PWD/.cache/base16/sequences"
for term in $terms
do
  # 16 color space
  put_template 0  $color00
  ...
done
...
```
 
I converted the Pywal `send` function into Bash, and wrapped the part of the shell script that sent the escape sequences. I also set it to save the sequences to a cache, to be run for each new terminal. This got me the exact terminal and Vim color swap I wanted. I set up a toggle script and assigned a hotkey using my window mananger `i3wm`. If I want to swap color palettes on other applications, I can add the necessary steps into the toggle script. I like knowing exactly what the toggle script is doing, vs. Pywal's "we'll try and take care of everything we can".

[The final result.](https://grant-uploader.s3.amazonaws.com/2024-06-19-21-34-47.gif)

I just modified the shell scripts for the specific gruvbox color schemes I wanted, but the cleaner way to do it would be to modify the shell template and regenerate them all. For now, I'm happy I got everything working and learned more about escape sequences and the structure of the Linux file system in the process.

## Lessons learned

Part of why I'm exploring Linux and scripting is to get a feel for how software could be more customizeable. A few things were especially interesting to me here:

1. Writing to all open terminals is a great example of the power of "everything is a file". Being able to locate all the open terminals and send the escape sequences to them through the file system interface shifted my mental model about scripting possibilities. I usually think of applications and files as very separate, and this blurred that a bit. I'd seen people talk about the power of the file concept before but this is one of the first times its been useful for something I was trying to do. I will spend some time thinking about how the file system concept could be applied to the software I make.
2. Escape sequences. I'm trying to think if you would ever want to include them (or a concept like them) in an application created from scratch. I don't think so. They're useful when you want to do formatting and the only interface you have with the program is that you can write text characters to it. The style is embedded in the text, but because the embedding is invisible it's going to be pretty unpredictable if you try and move it between programs.
3. The power of plain text and the being able to manipulate plain text. Lots of the config files for Linux applications are in a simple, plain text format. Coming from Javascript, I'm more used to intaking data as JSON and doing the manipulation in Javascript. In Linux you're more likely to manipulate the text directly, and there's a bunch of tools to help you do this. I'm sure that in some respects this leads to more formatting edge-case errors, but there's also a beauty to the simplicity. You can see this in how Pywal handles changing color config for a lot of applications: generate a color config in the proper format, then just include that in the larger appplication configuration. 
]]></description>
            <link>https://garden.grantcuster.com/2020-07-12-21-24-56-Swapping-color-schemes-across-all-terminals-and-Vim-with-Pywal-and-Base16</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2020-07-12-21-24-56-Swapping-color-schemes-across-all-terminals-and-Vim-with-Pywal-and-Base16</guid>
            <pubDate>Sun, 12 Jul 2020 21:24:56 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[2020-06-25-14-20-43-Vimlike]]></title>
            <description><![CDATA[# Vimlike

This [thread](https://twitter.com/helvetica/status/1274450330726645762), by Zach Gage, on how genre conventions serve as interaction shortcuts, got me thinking about how I use Vim conventions in my creative tools at [Constraint Systems](https://constraint.systems).

> 7/ A big part of making games involves working with genre literacy. In game design a key concept is the idea of weight: Every rule you add has a cognitive load on the player, and you must balance the weight of your rules against how meaningful they are to the play experience.

> 8/ An idea might be great, but if it makes the game unwieldy, ditch it. But genre-conventions are different -- they're weightless. They allow for an increased complexity and nuance in games, because they let designers include a huge number of rules without adding any weight.

Almost all of the experiments on Constraint Systems use Vim conventions: at least the `hjkl` characters for movement. One of the big reasons I started the experiments was my fascination with how I felt using Vim in the terminal. The combination of a strict character grid and keyboard controls provide a feeling of stability, and through that calm, that I don't feel in other programs, or using a computer in general.

This was especially in contrast to how I've felt when making gestural interface, or ones that simulate physics. Building those often felt like piling on edge-case handler on top of edge-case handler. If you did it well you could make a pleasing user experience, as long as they stuck to the path you had prepared. If they wanted to go a different direction, or you wanted to take the program in a new direction, you had to deal with that unwieldy tower, either by rearchitecting it or by adding even more code for handling the new edge cases.

I wanted to strip things down, and see if I could start from a more stable foundations, and I turned to Vim conventions to do that. It was a natural choice because I was chasing that feeling from Vim. Choosing Vim also gave me the interaction bootstrapping effect that Zach is talking about. Rather than asking the user to start from interaction scratch, I had the Vim foundation. That's not directly relevant for the majority of people, Vim is only used a subset of programmers, so it doesn't solve everything, but it is a place to start.

Even for users not familiar with Vim conventions, I think there's a benefit to starting the experiments there, rather than trying to introduce a new paradigm. Vim has proven itself to at least be useful to many people (and inspired a lot of loyalty). So there's an implicit promise that even if this looks weird, you know it can be learned and at least some people have found it useful.

There's a whole series of [Vim-like programs](https://reversed.top/2016-08-13/big-list-of-vim-like-software/), mostly terminal-based, that use similar key combinations. There's also a number of browser extensions that let you [use Vim keybindings in the browser](https://vim.fandom.com/wiki/Vim_key_bindings_for_web_browsers). Tiling window managers (I use [i3wm](https://i3wm.org/)) also share a lot of conventions. Putting these all together, you can put together a system for daily use that is keyboard-focused and mostly Vim-based. I've started referring to the Constraint Systems experiments as "alternative" interfaces. Vimlike interfaces are arguably the longest running, most fully fleshed out alternative interface for computers. I want to add to and learn from that system, and keep it alive in the face of the conventions (often imported from mobile/touchscreen design) that are dominating today.
]]></description>
            <link>https://garden.grantcuster.com/2020-06-25-14-20-43-Vimlike</link>
            <guid isPermaLink="true">https://garden.grantcuster.com/2020-06-25-14-20-43-Vimlike</guid>
            <pubDate>Thu, 25 Jun 2020 14:20:43 GMT</pubDate>
        </item>
    </channel>
</rss>